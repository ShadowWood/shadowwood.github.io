<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="icon" href="/images/glasses.svg" type="image/svg+xml"/>
    <link rel="shortcut icon" href="/images/glasses.svg" type="image/svg+xml"/>

    <!--Description-->
    
        <meta name="description" content="本文将介绍python基本的网络请求方式，以及网页抓包分析，适合于刚开始接触python的人学习">
    

    <!--Author-->
    
        <meta name="author" content="Shadow Wood">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="python网络爬虫入门"/>
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="ShadowWood&#39;s Blog"/>

    <!--Page Cover-->
    
        <meta property="og:image" content=""/>
    

    <!-- Title -->
    
    <title>python网络爬虫入门 - ShadowWood&#39;s Blog</title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/sass/main.css">

    <!--[if lt IE 8]>
        <script src="/js/ie/html5shiv.js"></script>
    <![endif]-->

    <!--[if lt IE 8]>
        <link rel="stylesheet" href="/sass/ie8.css">
    <![endif]-->

    <!--[if lt IE 9]>
        <link rel="stylesheet" href="/sass/ie9.css">
    <![endif]-->

    <!-- Gallery -->
    <link rel="stylesheet" href="/css/featherlight.min.css">
    <!-- Google Analytics -->
    


<script>
  var _hmt = _hmt || [];
  (function() {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?818f68a34fb7ee2d3343ec7bce3a06d5";
    var s = document.getElementsByTagName("script")[0]; 
    s.parentNode.insertBefore(hm, s);
  })();
</script>



</head>

<body>

    <div id="wrapper">

        <!-- Menu -->
        <!-- Header -->
<header id="header">
    <div class="inner">

        <!-- Logo -->
        <a href="/" class="logo">
            <span class="symbol"><img src="/images/glasses.svg" alt="" /></span><span class="title">ShadowWood's Blog</span>
        </a>

        <!-- Nav -->
        <nav>
            <ul>
                <li><a href="#menu">Menu</a></li>
            </ul>
        </nav>

    </div>
</header>

<!-- Menu -->
<nav id="menu">
    <h2>Menu</h2>
    <ul>
        
            <li>
                <a href="/">Home</a>
            </li>
        
            <li>
                <a href="/archives">Archives</a>
            </li>
        
            <li>
                <a href="/tags">Tags</a>
            </li>
        
            <li>
                <a href="/categories">Categories</a>
            </li>
        
            <li>
                <a href="/about">About</a>
            </li>
        
    </ul>
</nav>


        <div id="main">
            <div class="inner">

                <!-- Main Content -->
                

    <h1 class="title">python网络爬虫入门</h1>
    <div class="meta">
        2015-02-10
    </div>


<!-- Tags -->



<div class="tags">
    <a href="/tags/python/" class="button small">python</a> <a href="/tags/网络爬虫/" class="button small">网络爬虫</a>
</div>




    <span class="image main"><img src="http://7xu027.com1.z0.glb.clouddn.com/web-spider.jpg" alt="" /></span>


<!-- Gallery -->


<!-- Content -->
<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>对于刚刚接触python的人并且没有什么编程基础的人来说，从网络爬虫开始入门是个不错的选择。<br>学习一门编程语言要从实践中去学习，必须要自己动手操作，光是看一系列的编程书籍，不动动手做几个实际的程序运行运行，是完全不可能掌握一门语言的。<br>就好比武侠小说里学习一门武功一样，光是看别人操练，自己不比划比划，难道就能练就一身好功夫么？这样的话以后出身江湖不被别人砍死都是万幸了。</p>
<p>网络爬虫是我学习python(甚至可以说学习编程)的第一个程序(“Hello World”什么的就算了把)。所以在此极力推荐初学python的人选择从网络爬虫作为入门学习的选择。<br><a id="more"></a></p>
<h3 id="网络爬虫简介"><a href="#网络爬虫简介" class="headerlink" title="网络爬虫简介"></a>网络爬虫简介</h3><p>简单来说网络爬虫就是按照一定的算法和规则，自动的抓取网络信息的程序或者脚本。比较常用的也是这里要给大家介绍的基于HTTP协议的抓取网页并从中提取特定信息的脚本。</p>
<p>我们平常用浏览器打开一个网页，都是想网页的服务器端发送一个请求，然后服务器端做一系列的判断和处理，返回给我们一个HTML文件，然后我们的浏览器再将HTML文件解析成可视化的视图供我们浏览和操作。而这里要给大家介绍的爬虫就是将获取到HTML文件中的字符进行匹配和分析，提取我们需要的数据。</p>
<h3 id="获取网页"><a href="#获取网页" class="headerlink" title="获取网页"></a>获取网页</h3><p>对于python的介绍和安装，百度或谷歌一搜一大片，这里我也不再做介绍，我们直接切入正题。</p>
<p>python作为一门脚本语言，自身集合了很多WEB操作的库，再结合其简洁的语法，往往只需要简单几行代码，就能完成一些复杂的请求和分析。</p>
<p>现在我们以抓取天涯论坛的帖子为例。</p>
<ul>
<li><a href="http://bbs.tianya.cn/post-culture-308737-1.shtml" target="_blank" rel="external">http://bbs.tianya.cn/post-culture-308737-1.shtml</a></li>
</ul>
<p>这是一篇天涯论坛上的小说，现在我们准备写一个简单的网络爬虫，把这个网页的帖子抓取下来。</p>
<pre><code>import urllib

url = &quot;http://bbs.tianya.cn/post-culture-308737-1.shtml&quot;
re_f = urllib.urlopen(url)
page = re_f.read()
print page
</code></pre><p>urllib为python系统库专门用来处理WEB请求的一个包，用这个包可以实现向服务器端发送HTTP请求。</p>
<p>代码运行的结果是一整串的HTML代码，也就是我们做访问网页的源代码,就这样我们像浏览器一样获取网页的请求就完成了。</p>
<h3 id="匹配字符串获取数据"><a href="#匹配字符串获取数据" class="headerlink" title="匹配字符串获取数据"></a>匹配字符串获取数据</h3><p>接下来我们要对获取到的网页信息做处理，只抓取我们需要的数据。这里以抓取文章标题为例。</p>
<pre><code>import urllib
import re

url = &quot;http://bbs.tianya.cn/post-culture-308737-1.shtml&quot;
re_f = urllib.urlopen(url)
page = re_f.read()
content = re.findall(&quot;&lt;title&gt;(.*?)_.*?&lt;/title&gt;&quot;, page)
print content[0]
</code></pre><p>这里使用了正则表达式，关于正则表达式我之后会写一篇文章详细介绍。</p>
<p>re为python系统库用来处理正则表达式匹配的一个包。通过对网页源码的分析，我们不难发现，我们所需要的文章标题在标签<title>之中，但是这个标签的具体内容是这样的：</title></p>
<pre><code>&lt;title&gt;男人密码:《妻子，请原谅我》(已出版)_舞文弄墨_天涯论坛&lt;/title&gt;
</code></pre><p>我们所需要的内容只有“男人密码:《妻子，请原谅我》(已出版)”，title标签和“<em>舞文弄墨</em>天涯论坛”我们都不需要，所以我们将 “<title>“与”<em>舞文弄墨</em>天涯论坛”之间的内容用”()”提取出来即可，其中.*?表示匹配任意字符串。</title></p>
<p>re.findall()返回的是一个列表，我们只需要将第一个元素提取出来即可。</p>
<p>到这里，仅仅只用几行代码，一个简单的小爬虫就完成了。</p>
<hr>
<p>上一部分主要讲解了如何使用python发送HTTP  GET 请求并简单处理获取到的数据，其中用到了 urllib 和 re 库。http协议常用的请求中除了 GET 方法，还有 POST 方法。POST 方法与 GET 方法的区别网上已经有很多讲解了，这里笔者也不再赘述，简单的来说 GET 方法用于信息获取，POST 方法可能会对服务器上的资源进行修改。本篇文章以发送手机短信为例，讲解如何发送http POST请求。</p>
<h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>这里笔者选用的短信服务商为云片网，关于云片网的api文档请移步这里：</p>
<ul>
<li><a href="http://www.yunpian.com/api/sms.html" target="_blank" rel="external">http://www.yunpian.com/api/sms.html</a></li>
</ul>
<p>在api文档里已经有调用短信接口的python代码示例，不过示例代码中使用的是urllib库。因此笔者在这里使用urllib2进行编写，这个库也是python的标准库之一，用来处理http请求非常方便。</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code># -*- coding: utf-8 -*-
import urllib2
import urllib


url = &quot;http://yunpian.com/v1/sms/send.json&quot;
data = urllib.urlencode({&apos;apikey&apos;: &quot;***********************&quot;,
                         &apos;mobile&apos;: &apos;****************&apos;,
                         &apos;text&apos;: &quot;【云片网】您的验证码是1234&quot;})
response = urllib2.urlopen(url, data)
print response.read()
</code></pre><h3 id="代码讲解"><a href="#代码讲解" class="headerlink" title="代码讲解"></a>代码讲解</h3><p>apikey 对应云片网账号的 apikey，mobile 为需要发送短信的手机号码。</p>
<p>urllib.urlencode() 是将字典或包含两个元素的列表或元组转换为url参数，读者可以print一下本例的 data，结果输出为:</p>
<pre><code>“mobile=15682513909&amp;text=%E3%80%90%E4%BA%91%E7%89%87%E7%BD%91%E3%80%91%E6%82%A8%E7%9A%84%E9%AA%8C%E8%AF%81%E7%A0%81%E6%98%AF1234&amp;apikey=d25fbaa9ed5025eef7b777ca73c56af3”(text输出格式为unicode)。
</code></pre><p>urllib2.urlopen()创建一个表示远程url的类文件对象，然后像本地文件一样操作这个类文件对象来获取远程数据。当只传入url参数时，该函数将使用HTTP GET向url发送请求,当同时传入url和data参数时，该函数则使用HTTP POST方法向url发送请求。返回为一个类文件对象，该对象提供如下方法：</p>
<ul>
<li><p>read(), readline(), readlines(), fileno(), close()：这些方法的使用方式与文件对象完全一样，读者可以自行了解python对于文件对象的处理；</p>
</li>
<li><p>info()：返回一个httplib.HTTPMessage对象，表示远程服务器返回的头信息；</p>
</li>
<li><p>getcode()：返回HTTP状态码；</p>
</li>
<li><p>geturl()：返回请求的url；</p>
</li>
</ul>
<h3 id="代码进阶"><a href="#代码进阶" class="headerlink" title="代码进阶"></a>代码进阶</h3><p>对于以上的代码urllib2也能做到，而且在使用上几乎没有区别，那么这里我们对代码进行一些修改：</p>
<pre><code># -*- coding: utf-8 -*-
import urllib2
import urllib


url = &quot;http://yunpian.com/v1/sms/send.json&quot;
data = urllib.urlencode({&apos;apikey&apos;: &quot;***********************&quot;,
                     &apos;mobile&apos;: &apos;****************&apos;,
                     &apos;text&apos;: &quot;【云片网】您的验证码是1234&quot;})
req = urllib2.Request(url, data)
response = urllib2.urlopen(req)
print response.read()
</code></pre><p>其中我们引入了Request对象，并且直接在urllib2.urlopen()中传入Request对象，代码依旧能执行。在python官方的urllib2文档中，有这么一句话“Open the URL url, which can be either a string or a Request object.” 对于传入的url参数，可以直接是字符串，也可以是Request对象，而在urllib之中没有这种用法。</p>
<p>对于Request对象，我们可以通过它来修改 HTTP 请求的 header 和 proxy，这个我们在之后的文章会讲解，这里先提出这个用法，以免在以后的使用和介绍中显得突兀。</p>
<p>以上代码的最终结果是一个json字符串，使用type()方法返回值为str。可以使用python的json库，使用json.loads()将json字符串转换为数据原本的格式，本例的最终原本的数据为字典。</p>
<hr>
<p>有了以上的基础，我们可以尝试实现一个简单的模拟登录</p>
<h3 id="什么是模拟登录"><a href="#什么是模拟登录" class="headerlink" title="什么是模拟登录"></a>什么是模拟登录</h3><p>在很多网站上都会有一套用户系统，那么肯定就免除不了会有登录操作，并且许多信息和操作只有在登录之后才能实现。目前网站上判断用户是否登录有大概两种方式：</p>
<ul>
<li><p>使用cookie来对应记录用户的登录状态以及有效时间</p>
</li>
<li><p>自定义一个token，发送给已经登录的用户，里面存储了用户的一些信息已经登录状态有效时间</p>
</li>
</ul>
<p>这两种方式的思想都大同小异吧，目前我们主要对使用cookie进行登录判定的网站举例讲解。</p>
<h3 id="需要准备的工具"><a href="#需要准备的工具" class="headerlink" title="需要准备的工具"></a>需要准备的工具</h3><p>用户抓包的工具：chrome、firefox的firebug插件。至于抓包工具如何使用，大家可以自行搜索。</p>
<h3 id="抓包"><a href="#抓包" class="headerlink" title="抓包"></a>抓包</h3><p>首先我们需要通过抓包工具获取如下三个信息：</p>
<ul>
<li><p>post请求的网址</p>
</li>
<li><p>post请求的参数</p>
</li>
<li><p>post请求的返回值</p>
</li>
</ul>
<p>然后我们就针对以上获取到的数据进行下一步操作</p>
<h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><p>以电子科技大学的信息门户登录为例，需要用到的包:urllib，urllib2，cookielib</p>
<pre><code># -*- coding: utf-8 -*-
import urllib
import cookielib
import urllib2


loginUrl = &quot;https://uis.uestc.edu.cn/amsever/UI/Login&quot;
postUrl = &quot;https://uis.uestc.edu.cn/amserver/UI/Login&quot;
# 初始化一个CookieJar来处理cookie信息
cookieJar = cookielib.CookieJar()

# 初始化一个opener
cJHandler = urllib2.HTTPCookieProcessor(cookieJar)
opener = urllib2.build_opener(cJHandler, urllib2.HTTPHandler)
urllib2.install_opener(opener)

# 用get请求访问网站，获取必要的cookie值

res1 = opener.open(loginUrl)

# 用post请求向url提交表单

postData = {&quot;IDToken0&quot;: &quot;&quot;,
            # 登录用户名
            &quot;IDToken1&quot;: &quot;这里填写你的登录用户名&quot;,
            # 登录密码
            &quot;IDToken2&quot;: &quot;这里填写你的登录密码&quot;,
            &quot;IDButton&quot;: &quot;Submit&quot;,
            &quot;goto&quot;: &quot;aHR0cDovL3BvcnRhbC51ZXN0Yy5lZHUuY24vbG9naW4ucG9ydGFs&quot;,
            &quot;encoded&quot;: &quot;true&quot;,
            &quot;gx_charset&quot;: &quot;UTF-8&quot;}
postData = urllib.urlencode(postData).encode(encoding=&apos;UTF8&apos;)
req = urllib2.Request(postUrl, postData)
response = opener.open(req)
print response.read()
</code></pre><p>不出意外的话，打印的结果就是登录成功后返回的网页。</p>
<h3 id="代码讲解-1"><a href="#代码讲解-1" class="headerlink" title="代码讲解"></a>代码讲解</h3><p>相对于之前的爬虫代码，这里多了一个cookie的管理，所以我们使用了cookielib包来生成一个CookieJar对象来自动处理返回的cookie值。</p>
<p>之后我们需要将获取到的cookie值写入我们的每一次http请求中，于是我们使用了urllib2.build_opener()来生成一个OpenerDirector对象，而在一个OpenerDirector里面有很多的处理类，我们称之为handler。这些handler可以帮我们完善我们的http请求，比如本例中的向http请求中加入获取到的cookie。在调用了urllib2.builde_opener()之后，我们需要再调用urllib2.install_opener()来将生成的OpenerDirector实例化，这样在之后我们就可以使用OpenerDirector的open方法来代替urllib2.urlopen()方法了(当然你之后继续使用urllib2.urlopen()也不会有问题)</p>
<p>在模拟登录操作成功之后，我们就可以通过get或者post请求直接访问获取我们登录后才能看到的信息了，如个人学籍信息等，不过一定要使用urllib2.urlopen()或者OpenerDirector.open()方法来发送请求，因为我们之后的操作必须使用到我们之前登录成功获取到的cookie值，所以我们必须用我们生成的OpenerDirector对象的open方法来发送请求（其实urllib2.urlopen()也是默认调用当前OpenerDirector对象的open()方法）。</p>
<h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>对于模拟登录的方法，实现方式不止以上的一种，不过基本思想都是一致的。归根结底就是用户先向服务器发送需要认证的信息，然后服务器返回给你一个身份标识，之后用户每次向服务器发送请求的时候，都将这个身份标识一起发给服务器，向服务器证明你是一个登录的合法用户，然后服务器返回给用户应该看到的和想看到的数据。</p>


<!-- Comments -->
<div>
    



    
<hr />
<h3>留言:</h3>
<div id="gitalk-container">

</div>
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<style>
.gt-container .gt-header-controls-tip,
.gt-container a,
.gt-container .gt-svg svg {
    color: #585858;
}

.gt-container .gt-btn,
.gt-container .gt-btn:hover {
    background-color: #FFF;
}
</style>
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
<script>
var gitalk = new Gitalk({
  clientID: '3c20f2c5396af1661cdd',
  clientSecret: '0d22e72a163265bb03f44e550c73d779bc47315f',
  repo: 'shadowwood.github.io',
  owner: 'ShadowWood',
  admin: [
      
        'ShadowWood',
      
  ],
  // facebook-like distraction free mode
  distractionFreeMode: true
})

gitalk.render('gitalk-container')
</script>


</div>



            </div>
        </div>

        <!-- Footer -->
<footer id="footer">
    <div class="inner">
        <section>
            <h2>FriendLinks</h2>
            <div>
                
                        <a href="https://www.rapospectre.com/" target="_blank">RaPoSpectre</a>&nbsp;&nbsp;
                    
                        <a href="https://vkwk.space/" target="_blank">Viking Warlock</a>&nbsp;&nbsp;
                    
                        <a href="http://djjowfy.com/" target="_blank">djjowfy</a>&nbsp;&nbsp;
                    
                        <a href="http://toxni.com/" target="_blank">Toxni</a>&nbsp;&nbsp;
                    
                        <a href="https://www.yasicyu.com/" target="_blank">Yasic Yu</a>&nbsp;&nbsp;
                    
            </div>
        </section>
        <section>
            <h2>Follow</h2>
            <ul class="icons">
                
                
                
                
                
                    <li><a href="https://github.com/ShadowWood" class="icon style2 fa-github" target="_blank" ><span class="label">GitHub</span></a></li>
                
                
                
                
                
                    <li><a href="shadowwood@foxmail.com" class="icon style2 fa-envelope-o" target="_blank" ><span class="label">Email</span></a></li>
                
                
                    <li><a href="/atom.xml" class="icon style2 fa-rss" target="_blank" ><span class="label">RSS</span></a></li>
                
            </ul>
        </section>
        <ul class="copyright">
            <li>&copy; 2016 - 2017 All rights reserved</li>
            <li>Design: <a href="http://html5up.net" target="_blank">HTML5 UP</a></li>
            <li>
                Hosted by 
                
                    <a href="https://pages.coding.me" style="font-weight: bold">
                        Coding Pages
                    </a>
                    &nbsp;
                
                    <a href="https://pages.github.com" style="font-weight: bold">
                        GitHub Pages
                    </a>
                    &nbsp;
                
            </li>
            <li>Shadow Wood</li>
        </ul>
    </div>
</footer>
    </div>

    <!-- After footer scripts -->
    
<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- skel -->
<script src="/js/skel.min.js"></script>

<!-- Custom Code -->
<script src="/js/util.js"></script>

<!--[if lte IE 8]>
<script src="/js/ie/respond.min.js"></script>
<![endif]-->

<!-- Custom Code -->
<script src="/js/main.js"></script>

<!-- Gallery -->
<!-- <script src="//cdn.rawgit.com/noelboss/featherlight/1.3.5/release/featherlight.min.js" type="text/javascript" charset="utf-8"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/detect_swipe/2.1.1/jquery.detect_swipe.min.js"></script> -->
<script src="/js/jquery.min.js"></script>
<script src="/js/featherlight.min.js"></script>

<!-- Disqus Comments -->

  

</body>

</html>